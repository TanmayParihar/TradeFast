# Mamba State Space Model Configuration
# Optimized for 8GB VRAM (RTX 4070)

model:
  name: mamba_trader
  type: custom

architecture:
  d_model: 64
  d_state: 64
  d_conv: 4
  expand: 2
  n_layers: 4
  dropout: 0.1
  use_bidirectional: false

input:
  seq_length: 512
  input_dim: 50
  embedding_dim: 64

output:
  num_classes: 3
  output_type: classification

training:
  learning_rate: 0.001
  batch_size: 64
  max_epochs: 100
  gradient_clip_val: 1.0
  use_gradient_checkpointing: true

loss:
  type: cross_entropy
  label_smoothing: 0.1
  class_weights: null

optimizer:
  name: AdamW
  betas: [0.9, 0.999]
  weight_decay: 0.01

scheduler:
  name: CosineAnnealingWarmRestarts
  T_0: 10
  T_mult: 2
  eta_min: 1e-6

regularization:
  dropout: 0.1
  weight_decay: 0.01
  gradient_noise: 0.0

initialization:
  method: xavier_uniform
  gain: 1.0
